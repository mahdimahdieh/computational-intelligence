\section{مفاهیم و حل مسئله}
\begin{enumerate}
	\item بله، هر نورون در یک شبکهٔ عصبی حامل نوعی اطلاعات است؛ اما ماهیت و میزان «وضوح» این اطلاعات بسته به عمق لایه و ویژگی‌های بنیادین شبکه متفاوت است. 
	
	
	 چهار ویژگی بنیادی و سلسله‌مراتبی بودن نمایش:
	
	\begin{enumerate}
		\item \textbf{توابع غیرخطی (Nonlinearity)}
		\begin{itemize}
			\item هر نورون پس از ترکیب خطی ورودی‌ها (ضرب وزن‌ها + بایاس) خروجی را از طریق تابعی مانند \lr{ReLU}، \lr{sigmoid} یا \lr{tanh} عبور می‌دهد.
			\item بدون غیرخطی‌سازی، شبکه عملاً یک عملگر خطی بزرگ خواهد بود و قادر به تشخیص زیرویژگی‌های پیچیده نیست.
			\item تابع فعال‌سازی باعث می‌شود هر نورون تنها در صورت وقوع یک الگوی خاص «فعال» شود و در نتیجه به‌عنوان یک تشخیص‌دهندهٔ ساده عمل کند.
		\end{itemize}
		
		\item \textbf{نمایش توزیع‌شده (\lr{Distributed Representation})}
		\begin{itemize}
			\item برخلاف سیستم‌های سمبلیک که هر مفهوم را با یک واحد منفرد نمایش می‌دهند، شبکه‌های عصبی مفاهیم را به‌صورت همزمان در بردار فعال‌سازی تعداد زیادی نورون کدگذاری می‌کنند.
			\item این پراکندگی اطلاعات باعث افزایش مقاومت شبکه در برابر نویز و آسیب به نورون‌های منفرد می‌شود.
			\item هر نورون سهم جزئی اما معنادار در تشخیص زیرویژگی‌های ساده یا انتزاعی دارد.
		\end{itemize}
		
		\item \textbf{یادگیری گرادیان‌محور (\lr{Gradient‐based Learning})}
		\begin{itemize}
			\item با استفاده از الگوریتم پس‌انتشار (\lr{Backpropagation})، وزن‌ها و بایاس هر نورون به‌روزرسانی می‌شود تا خطای خروجی به کمترین مقدار برسد.
			\item در طی آموزش، هر نورون به زیرویژگی‌هایی پاسخ می‌دهد که برای کاهش خطا در مسئلهٔ مشخص مفیدند.
			\item در پایان آموزش، وزن‌های ورودی هر نورون تعیین می‌کنند که آن نورون به چه الگو یا ویژگی‌ حساس باشد.
		\end{itemize}
		
		\item \textbf{سلسله‌مراتب ویژگی‌ها (\lr{Hierarchical Feature Learning})}
		\begin{itemize}
			\item لایه‌های ابتدایی شبکه‌های عمیق معمولاً به زیرویژگی‌های ساده مانند لبه‌های عمودی/افقی یا بافت‌ها حساس‌اند.
			\item لایه‌های میانی ترکیب این زیرویژگی‌ها را انجام داده و الگوهای پیچیده‌تر  را می‌آموزند.
			\item در لایهٔ خروجی (مثلاً نورون‌های \lr{softmax}) احتمال تعلق هر ورودی به یک کلاس نهایی (مثلاً «گربه» یا «سگ») کدگذاری می‌شود.
		\end{itemize}
	\end{enumerate}
	
	\item در شبکه‌های عصبی، «دانش» در قالب پارامترها (وزن‌ها و بایاس‌ها) ذخیره می‌شود و از طریق فرآیند آموزش شکل می‌گیرد؛ در ادامه، یک پاسخ یکپارچه و مرتب‌شده ارائه شده است:
	
	\begin{enumerate}
		\item \textbf{شکل‌گیری دانش در شبکه‌های عصبی}
		\begin{enumerate}
			\item \textbf{تعریف ساختار شبکه (\lr{Architecture}):}  
			انتخاب تعداد لایه‌ها (\lr{Input, Hidden, Output})، نوع آن‌ها (\lr{fully-connected}، کانولوشن، بازگشتی و …) و تعداد نورون در هر لایه.
			\item \textbf{مقداردهی اولیه پارامترها (\lr{Initialization}):}  
			وزن‌ها و بایاس‌ها معمولاً با توزیع‌های تصادفی (مثل \lr{Xavier} یا \lr{He}) مقداردهی می‌شوند.
			\item \textbf{انتشار رو به جلو (\lr{Forward Propagation}):}  
			برای هر ورودی \(x\)، در هر لایه:
			\[
			z^{(\ell)} = W^{(\ell)}\,a^{(\ell-1)} + b^{(\ell)}, 
			\quad
			a^{(\ell)} = \sigma\bigl(z^{(\ell)}\bigr)
			\]
			در نهایت \(a^{(L)}\) خروجی نهایی شبکه است.
			\item \textbf{محاسبه خطا (\lr{Loss Calculation}):}  
			با تابع هزینه \(L\bigl(y_{\text{pred}},\,y_{\text{true}}\bigr)\)، مانند MSE برای رگرسیون یا Cross-Entropy برای طبقه‌بندی.
			\item \textbf{پس انتشار خطا (\lr{Backpropagation}):}  
			مشتق تابع هزینه را نسبت به پارامترها محاسبه می‌کنیم:
			\[
			\frac{\partial L}{\partial W^{(\ell)}},\quad
			\frac{\partial L}{\partial b^{(\ell)}}
			\]
			\item \textbf{به‌روزرسانی پارامترها (\lr{Optimization}):}  
			با الگوریتم‌هایی مثل \lr{Gradient Descent} یا \lr{Adam}:
			\[
			W^{(\ell)} \leftarrow W^{(\ell)} - \eta\,\frac{\partial L}{\partial W^{(\ell)}}, 
			\quad
			b^{(\ell)} \leftarrow b^{(\ell)} - \eta\,\frac{\partial L}{\partial b^{(\ell)}}
			\]
			این چرخه تا رسیدن به همگرایی تکرار می‌شود.
		\end{enumerate}
		
		\item \textbf{فرمول‌بندی «معادل بودن» دو شبکه عصبی}
		\begin{enumerate}
			\item \textbf{معادل تابعی (\lr{Exact Functional Equivalence})}  
			دو شبکه \(N(x)=f_{\theta_N}(x)\) و \(M(x)=f_{\theta_M}(x)\) دقیقاً معادل‌اند اگر:
			\[
			\forall x\in X,\quad N(x)=M(x).
			\]
			\item \emph{معادل ساختاری تحت تبدیلات (\lr{Structural Equivalence})}  
			در لایه‌های \lr{Dense}، جابجایی نورون‌ها (پرموتیشن \(\pi\)) همراه با جابجایی سطرها/ستون‌های متناظر در \(W,b\)، خروجی را تغییر نمی‌دهد.
			\item \emph{تقریب معادل (\lr{Approximate Equivalence})}  
			با فاصلهٔ خروجی \(d(x)=\|N(x)-M(x)\|_p\):
			\[
			\forall x\in X,\;d(x)<\epsilon
			\quad\text{یا}\quad
			\mathrm{KL}\bigl(N(x)\Vert M(x)\bigr)<\delta.
			\]
		\end{enumerate}
		
		\item \textbf{مثال ریاضی}
		\begin{enumerate}
			\item \textbf{حالت سادهٔ خطی:}  
			دو شبکه خطی با یک لایه پنهان و \(\sigma(z)=z\):
			\[
			N(x)=W_2\,(W_1\,x+b_1)+b_2,\quad
			M(x)=W_2'\,(W_1'\,x+b_1')+b_2'.
			\]
			آن‌ها معادل‌اند اگر:
			\[
			W_2W_1 = W_2'W_1', 
			\quad
			W_2b_1 + b_2 = W_2'b_1' + b_2'.
			\]
			\item \emph{اشاره‌ای به حالت غیرخطی:}  
			در شبکه‌های غیرخطی (مثلاً \lr{ReLU})، تبدیلات پیچیده‌ترند؛ اما با ادغام \lr{BatchNorm} یا تبدیلات جبری می‌توان مشابهت رفتار را نشان داد.
		\end{enumerate}
		
	\end{enumerate}
	
	
	
	%%%%%%%%%%%% سوال ۶ %%%%%%%%%%%
	\item 
	
	\begin{enumerate}
		\item {\bf طراحی پرسپترون تک‌لایه}
		
		فرض کنیم می‌خواهیم الگوها را به صورت برچسب
		\(\;t_1=-1\) برای \(P_1\) و \(t_2=+1\) برای \(P_2\) دسته‌بندی کنیم.
		باید \(w\in\mathbb{R}^3\) و بایاس \(b\) را طوری بیابیم که
		\[
		\begin{cases}
			\operatorname{sign}(w^\top P_1 + b) = -1,\\
			\operatorname{sign}(w^\top P_2 + b) = +1.
		\end{cases}
		\]
		این معادلات به صورت نابرابری‌های زیر نوشته می‌شوند:
		\[
		w^\top\!(-1,-1,1) + b < 0,
		\quad
		w^\top\!(+1,-1,1) + b > 0.
		\]
		به سادگی می‌توانیم مثلاً وزن‌ها را به صورت
		\(\;w = (1,0,0)\;\)، و بایاس \(b=0\) انتخاب کنیم:
		\[
		w^\top P_1 + b = -1 < 0,\quad
		w^\top P_2 + b = +1 > 0.
		\]
		لذا تابع تصمیم
		\(\;y = \operatorname{sign}(x_1)\)
		دو الگو را به درستی تفکیک می‌کند.
		
		\item {\bf طراحی شبکه Hamming}
		
		شبکه همینگ برای \(N\) الگو \(P_k\) به صورت زیر است:
		\[
		\textbf{W} = 
		\begin{bmatrix}
			P_1^\top \\ P_2^\top
		\end{bmatrix}, 
		\quad
		y = \arg\max_{k}\bigl(\textbf{W}\,x\bigr)_k.
		\]
		برای \(P_1,P_2\) داریم:
		\[
		\textbf{W} =
		\begin{pmatrix}
			-1 & -1 & 1\\
			+1 & -1 & 1
		\end{pmatrix},
		\quad
		\text{انتخاب \(k\) با بیشینه‌ی \(\sum_i W_{k,i}x_i\).}
		\]
		
		\item {\bf طراحی شبکه Hopfield}
		
		شبکه هاپفیلد با الگوهای باینری \(\pm1\) به کمک قاعده
		\(\;T = \sum_k P_k P_k^\top\;\) ساخته می‌شود. اینجا داریم:
		\[
		T \;=\; P_1P_1^\top + P_2P_2^\top
		\;=\;
		\begin{pmatrix}
			1 & 1 & -1\\
			1 & 1 & -1\\
			-1 & -1 & 1
		\end{pmatrix}
		+
		\begin{pmatrix}
			1 & -1 & 1\\
			-1 & 1 & -1\\
			1 & -1 & 1
		\end{pmatrix}
		=
		\begin{pmatrix}
			2 & 0 & 0\\
			0 & 2 & -2\\
			0 & -2 & 2
		\end{pmatrix}.
		\]
		سپس حالت نرونی‌ها با قاعده
		\(\;x_i \leftarrow \operatorname{sign}\bigl(\sum_j T_{ij}x_j\bigr)\;\)
		به سمت نزدیک‌ترین الگو جذب می‌شود.
		
	\end{enumerate}
	
	%%%%%% سوال ۷ %%%%%%%
	\item
	
	\begin{enumerate}
		\item 
		\textbf{طراحی مرز تصمیم و شبکه پرسپترون تک‌لایه}
		
		
		با انتخاب وزن‌ها و بایاس زیر:
		\[
		\mathbf{w} = \begin{bmatrix}-1\\-1\end{bmatrix},
		\quad b = \frac{1}{2}
		\]
		تابع فعال‌سازی گام به این صورت خواهد بود:
		\[
		y = 
		\begin{cases}
			1, & \mathbf{w}^\top \mathbf{x} + b > 0,\\
			0, & \text{وگرنه}.
		\end{cases}
		\]
		\\
		\textbf{معادله مرز تصمیم:}
		\[
		-x_1 - x_2 + \frac{1}{2} = 0
		\quad\Longleftrightarrow\quad
		x_1 + x_2 = \frac{1}{2}.
		\]
		
		\begin{center}
			\begin{tikzpicture}[scale=1]
				\draw[->] (-2,0) -- (3,0) node[right] {$x_1$};
				\draw[->] (0,-2) -- (0,3) node[above] {$x_2$};
				% نقاط مثبت
				\fill ( -1,  1) circle[radius=2pt] node[above left] {$P_1$};
				\fill (  0,  0) circle[radius=2pt] node[below right] {$P_2$};
				\fill (  1, -1) circle[radius=2pt] node[below right] {$P_3$};
				% نقاط منفی
				\draw[fill=white] ( 1, 0) circle[radius=2pt] node[below right] {$P_4$};
				\draw[fill=white] ( 0, 1) circle[radius=2pt] node[above left] {$P_5$};
				\draw[fill=white] ( 1, 2) circle[radius=2pt] node[right] {$P_6$};
				% مرز تصمیم
				\draw[thick,blue] (-1.5,2) -- (2,-1.5) node[right] {$x_1 + x_2 = \frac{1}{2}$};
			\end{tikzpicture}
		\end{center}
		
		\item 
		\textbf{تشخیص قابلیت جداسازی و تعیین بازه \(\varepsilon\)}
		
		از‌ نامعادلات زیر برای کلاس بندی استفاده می‌کنیم:
		\[
		\begin{cases}
			-x_1 - x_2 + b > 0 & 1\\
			-x_1 - x_2 + b < 0 & 0
		\end{cases}
		\]
		نتیجه می‌شود که برای هر \(\varepsilon\ge0\) می‌توان \(w_1=w_2=-1\) و \(b\in(0,1)\) (مثلاً \(b=1\)) را انتخاب کرد و جداسازی خطی امکان‌پذیر است.
		
		\item 
		\textbf{اجرای الگوریتم پرسپترون و نتایج نهایی}
		
		برای سه مقدار \(\varepsilon\) اجرای الگوریتم با نرخ یادگیری \(\eta=1\)، وزن و بایاس را از صفر مقداردهی کرده و تا خطای صفر تکرار می‌کنیم.
		
		\lstinputlisting[language=Python, caption= پیاده‌سازی الگوریتم پرسپترون]{scripts/epsilon.py}

		
		{\small
			\[
			\begin{aligned}
				&\varepsilon=1:\quad \mathbf{w}=(-1,\,-1),\; b=1,\;\text{epochs}=2,\\
				&\varepsilon=2:\quad \mathbf{w}=(-2,\,-2),\; b=1,\;\text{epochs}=4,\\
				&\varepsilon=6:\quad \mathbf{w}=(-3,\,-4),\; b=3,\;\text{epochs}=4.
			\end{aligned}
			\]}
		
		\item 
		\textbf{خلاصه نتایج}
		
		\begin{itemize}
			\item مرز تصمیم: \(x_1 + x_2 = \frac{1}{2}\).
			\item بازه \(\varepsilon\): \(\varepsilon \ge 0\) (تمام مقادیر غیرمنفی).
			\item وزن‌ها و بایاس نهایی برای \(\varepsilon=1,2,6\) مطابق جدول فوق.
			\item الگوریتم پرسپترون حداکثر تا ۴ دور همگرا شده و خطای صفر حاصل شد.
		\end{itemize}
		
	\end{enumerate}
\end{enumerate}
