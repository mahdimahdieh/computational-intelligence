\section{مفاهیم و حل مسئله}
\begin{enumerate}
	\item بله، هر نورون در یک شبکهٔ عصبی حامل نوعی اطلاعات است؛ اما ماهیت و میزان «وضوح» این اطلاعات بسته به عمق لایه و ویژگی‌های بنیادین شبکه متفاوت است. 
	
	
	 چهار ویژگی بنیادی و سلسله‌مراتبی بودن نمایش:
	
	\begin{enumerate}
		\item \textbf{توابع غیرخطی (Nonlinearity)}
		\begin{itemize}
			\item هر نورون پس از ترکیب خطی ورودی‌ها (ضرب وزن‌ها + بایاس) خروجی را از طریق تابعی مانند \lr{ReLU}، \lr{sigmoid} یا \lr{tanh} عبور می‌دهد.
			\item بدون غیرخطی‌سازی، شبکه عملاً یک عملگر خطی بزرگ خواهد بود و قادر به تشخیص زیرویژگی‌های پیچیده نیست.
			\item تابع فعال‌سازی باعث می‌شود هر نورون تنها در صورت وقوع یک الگوی خاص «فعال» شود و در نتیجه به‌عنوان یک تشخیص‌دهندهٔ ساده عمل کند.
		\end{itemize}
		
		\item \textbf{نمایش توزیع‌شده (\lr{Distributed Representation})}
		\begin{itemize}
			\item برخلاف سیستم‌های سمبلیک که هر مفهوم را با یک واحد منفرد نمایش می‌دهند، شبکه‌های عصبی مفاهیم را به‌صورت همزمان در بردار فعال‌سازی تعداد زیادی نورون کدگذاری می‌کنند.
			\item این پراکندگی اطلاعات باعث افزایش مقاومت شبکه در برابر نویز و آسیب به نورون‌های منفرد می‌شود.
			\item هر نورون سهم جزئی اما معنادار در تشخیص زیرویژگی‌های ساده یا انتزاعی دارد.
		\end{itemize}
		
		\item \textbf{یادگیری گرادیان‌محور (\lr{Gradient‐based Learning})}
		\begin{itemize}
			\item با استفاده از الگوریتم پس‌انتشار (\lr{Backpropagation})، وزن‌ها و بایاس هر نورون به‌روزرسانی می‌شود تا خطای خروجی به کمترین مقدار برسد.
			\item در طی آموزش، هر نورون به زیرویژگی‌هایی پاسخ می‌دهد که برای کاهش خطا در مسئلهٔ مشخص مفیدند.
			\item در پایان آموزش، وزن‌های ورودی هر نورون تعیین می‌کنند که آن نورون به چه الگو یا ویژگی‌ حساس باشد.
		\end{itemize}
		
		\item \textbf{سلسله‌مراتب ویژگی‌ها (\lr{Hierarchical Feature Learning})}
		\begin{itemize}
			\item لایه‌های ابتدایی شبکه‌های عمیق معمولاً به زیرویژگی‌های ساده مانند لبه‌های عمودی/افقی یا بافت‌ها حساس‌اند.
			\item لایه‌های میانی ترکیب این زیرویژگی‌ها را انجام داده و الگوهای پیچیده‌تر  را می‌آموزند.
			\item در لایهٔ خروجی (مثلاً نورون‌های \lr{softmax}) احتمال تعلق هر ورودی به یک کلاس نهایی (مثلاً «گربه» یا «سگ») کدگذاری می‌شود.
		\end{itemize}
	\end{enumerate}
	
	\item در شبکه‌های عصبی، «دانش» در قالب پارامترها (وزن‌ها و بایاس‌ها) ذخیره می‌شود و از طریق فرآیند آموزش شکل می‌گیرد؛ در ادامه، یک پاسخ یکپارچه و مرتب‌شده ارائه شده است:
	
	\begin{enumerate}
		\item \textbf{شکل‌گیری دانش در شبکه‌های عصبی}
		\begin{enumerate}
			\item \textbf{تعریف ساختار شبکه (\lr{Architecture}):}  
			انتخاب تعداد لایه‌ها (\lr{Input, Hidden, Output})، نوع آن‌ها (\lr{fully-connected}، کانولوشن، بازگشتی و …) و تعداد نورون در هر لایه.
			\item \textbf{مقداردهی اولیه پارامترها (\lr{Initialization}):}  
			وزن‌ها و بایاس‌ها معمولاً با توزیع‌های تصادفی (مثل \lr{Xavier} یا \lr{He}) مقداردهی می‌شوند.
			\item \textbf{انتشار رو به جلو (\lr{Forward Propagation}):}  
			برای هر ورودی \(x\)، در هر لایه:
			\[
			z^{(\ell)} = W^{(\ell)}\,a^{(\ell-1)} + b^{(\ell)}, 
			\quad
			a^{(\ell)} = \sigma\bigl(z^{(\ell)}\bigr)
			\]
			در نهایت \(a^{(L)}\) خروجی نهایی شبکه است.
			\item \textbf{محاسبه خطا (\lr{Loss Calculation}):}  
			با تابع هزینه \(L\bigl(y_{\text{pred}},\,y_{\text{true}}\bigr)\)، مانند MSE برای رگرسیون یا Cross-Entropy برای طبقه‌بندی.
			\item \textbf{پس انتشار خطا (\lr{Backpropagation}):}  
			مشتق تابع هزینه را نسبت به پارامترها محاسبه می‌کنیم:
			\[
			\frac{\partial L}{\partial W^{(\ell)}},\quad
			\frac{\partial L}{\partial b^{(\ell)}}
			\]
			\item \textbf{به‌روزرسانی پارامترها (\lr{Optimization}):}  
			با الگوریتم‌هایی مثل \lr{Gradient Descent} یا \lr{Adam}:
			\[
			W^{(\ell)} \leftarrow W^{(\ell)} - \eta\,\frac{\partial L}{\partial W^{(\ell)}}, 
			\quad
			b^{(\ell)} \leftarrow b^{(\ell)} - \eta\,\frac{\partial L}{\partial b^{(\ell)}}
			\]
			این چرخه تا رسیدن به همگرایی تکرار می‌شود.
		\end{enumerate}
		
		\item \textbf{فرمول‌بندی «معادل بودن» دو شبکه عصبی}
		\begin{enumerate}
			\item \textbf{معادل تابعی (\lr{Exact Functional Equivalence})}  
			دو شبکه \(N(x)=f_{\theta_N}(x)\) و \(M(x)=f_{\theta_M}(x)\) دقیقاً معادل‌اند اگر:
			\[
			\forall x\in X,\quad N(x)=M(x).
			\]
			\item \emph{معادل ساختاری تحت تبدیلات (\lr{Structural Equivalence})}  
			در لایه‌های \lr{Dense}، جابجایی نورون‌ها (پرموتیشن \(\pi\)) همراه با جابجایی سطرها/ستون‌های متناظر در \(W,b\)، خروجی را تغییر نمی‌دهد.
			\item \emph{تقریب معادل (\lr{Approximate Equivalence})}  
			با فاصلهٔ خروجی \(d(x)=\|N(x)-M(x)\|_p\):
			\[
			\forall x\in X,\;d(x)<\epsilon
			\quad\text{یا}\quad
			\mathrm{KL}\bigl(N(x)\Vert M(x)\bigr)<\delta.
			\]
		\end{enumerate}
		
		\item \textbf{مثال ریاضی}
		\begin{enumerate}
			\item \textbf{حالت سادهٔ خطی:}  
			دو شبکه خطی با یک لایه پنهان و \(\sigma(z)=z\):
			\[
			N(x)=W_2\,(W_1\,x+b_1)+b_2,\quad
			M(x)=W_2'\,(W_1'\,x+b_1')+b_2'.
			\]
			آن‌ها معادل‌اند اگر:
			\[
			W_2W_1 = W_2'W_1', 
			\quad
			W_2b_1 + b_2 = W_2'b_1' + b_2'.
			\]
			\item \emph{اشاره‌ای به حالت غیرخطی:}  
			در شبکه‌های غیرخطی (مثلاً \lr{ReLU})، تبدیلات پیچیده‌ترند؛ اما با ادغام \lr{BatchNorm} یا تبدیلات جبری می‌توان مشابهت رفتار را نشان داد.
		\end{enumerate}
		
	\end{enumerate}
	

	\item در شبکه‌های عصبی، توانایی یادگیری، به‌خاطر سپاری (\lr{Memorization}) و تعمیم (\lr{Generalization}) بر پایه‌ی ساختار معماری، الگوریتم‌های آموزش و ویژگی‌های داده‌ها شکل می‌گیرد. در ادامه، این سه قابلیت را همراه با مبانی ریاضی و مثال‌های عینی بررسی می‌کنیم.
	
	\begin{enumerate}
		\item \textbf{یادگیری (\lr{Learning})}\\
		شبکه با کمینه‌سازی تابع هزینه
		\[
		L(\theta) = \frac{1}{N} \sum_{i=1}^{N} \ell\bigl(f_\theta(x_i), y_i\bigr)
		\]
		و به‌کارگیری انتشار رو به جلو و پسانتشار خطا (Backpropagation)، پارامترهای \(\theta\) (وزن‌ها و بایاس‌ها) را با الگوریتم‌های گرادیان‌محور (SGD، Adam و…) به‌روزرسانی می‌کند:
		\[
		\theta \;\leftarrow\; \theta - \eta\,\nabla_\theta L(\theta).
		\]
		\begin{itemize}
			\item \textbf{قضیه تقریب جهانی (\lr{Universal Approximation Theorem})}  
			هر شبکه‌ی با حداقل یک لایه پنهان و تابع فعال‌سازی غیرفابی (مثلاً \lr{ReLU} یا \lr{Sigmoid}) می‌تواند هر تابع پیوسته روی یک مجموعه‌ی کامپکت را تقریب بزند.
			\item \textbf{ویژگی توزیعی (\lr{Distributed Representation})}  
			هر نورون یا زیراسختار فقط بخشی از ویژگی‌های داده را مدل می‌کند و با ترکیب میلیون‌ها پارامتر، شبکه قادر به نمایش الگوهای غیرخطی و سلسله‌مراتبی است.
		\end{itemize}
		
		\item \textbf{به‌خاطر سپاری (\lr{Memorization})}\\
		شبکه‌های \lr{overparameterized} (پارامترها خیلی بزرگتر از نمونه‌ها) می‌توانند جزئیات حتی نویزی داده‌های آموزشی را حفظ کنند:
		\[
		\text{High Capacity: VC-Dimension و Rademacher Complexity بزرگ.}
		\]
		\begin{itemize}
			\item \textbf{مثال \lr{Bias–Variance Tradeoff}:}  
			\[
			\text{Complexity}\!\uparrow \;\to\; \text{Bias}\!\downarrow,\;\text{Variance}\!\uparrow,\;\text{Memorization}\!\uparrow
			\]
			اگر هیچ ضابطه‌ای (\lr{Regularization}) وجود نداشته باشد، شبکه می‌تواند اطلاعات آموزشی را تقریباً کامل بازتولید کند.
		\end{itemize}
		
		\item \textbf{تعمیم (\lr{Generalization})}\\
		تعمیم یعنی عملکرد خوب روی داده‌های ندیده. این امر با ترکیب مکانیزم‌های \lr{implicit} و \lr{explicit regularization} و \lr{Inductive Bias} حاصل می‌شود:
		\[
		L_{\text{reg}}(\theta) = L(\theta) + \lambda \|\theta\|_2^2
		\]
		\begin{itemize}
			\item \textbf{\lr{\lr{Implicit Regularization}}:} رفتار SGD شبکه را به سمت مینیمم‌های تخت (\lr{flat minima}) هدایت می‌کند.
			\item \textbf{\lr{Regularization} صریح:}
			\begin{itemize}
				\item \lr{Weight Decay} (\lr{L2}): افزودن \(\lambda\|\theta\|_2^2\) به تابع هزینه.
				\item \lr{Dropout}: غیرفعال‌سازی تصادفی نورون‌ها.
				\item \lr{Early Stopping}: پایان آموزش پیش از شروع شدید Overfitting.
			\end{itemize}
			\item \textbf{\lr{Inductive Bias} معماری:}
			\begin{itemize}
				\item \lr{CNN}: اشتراک وزن‌ها و حساسیت به ویژگی‌های مکانی.
				\item \lr{RNN/Transformer}: نگاشت توالی‌های زمانی و وابستگی‌های ترتیبی.
			\end{itemize}
			\item \textbf{نرمال‌سازی (\lr{Batch Normalization})}  
			کاهش حساسیت به مقیاس وزن‌ها و تثبیت جریان گرادیان.
			\item \textbf{داده‌های متنوع و کافی}  
			کمیت و کیفیت داده‌های آموزشی پایه‌ی استخراج قاعده‌های عمومی و کاهش \lr{Overfitting} است.
		\end{itemize}
		
		\item \textbf{پیوند با «معادل بودن دو شبکه» و «شکل‌گیری دانش»}\\
		شکل‌گیری دانش = پارامترهای بهینه \(\theta\) که از فرآیند یادگیری به‌دست می‌آیند.  
		\emph{معادل بودن دو شبکه} وقتی است که:
		\[
		\forall x,\; N(x) = M(x)
		\quad\text{(Exact Functional)}
		\]
		یا با پرموتیشن \(\pi\) روی نورون‌ها (\lr{Structural Symmetry})، یا تقریباً:
		\[
		\|N(x)-M(x)\|_p < \varepsilon
		\quad\text{یا}\quad
		\mathrm{KL}\bigl(N(x)\,\|\,M(x)\bigr) < \delta.
		\]
	\end{enumerate}
	
	\newpage
	
	\item در زیر سه نوع رایج از “توابع تبدیل نورونی” (یا به‌عبارت دیگر انواع نورون‌های مرتبه‌بالا و \lr{RBF}) را به‌صورت ریاضی می‌بینید:
	\begin{enumerate}
		\item \textbf{نورون درجه دوم (\lr{Quadratic Neuron}):}
		\[
		\begin{aligned}
			\mathrm{net}(\mathbf{x})
			&= \sum_{i=1}^n \sum_{j=1}^n w_{ij}\,x_i\,x_j
			+ \sum_{i=1}^n v_i\,x_i
			+ b,\\
			y &= f\bigl(\mathrm{net}(\mathbf{x})\bigr),
		\end{aligned}
		\]
		که در آن
		\begin{itemize}
			\item $\mathbf{x}=(x_1,\dots,x_n)\in\mathbb{R}^n$،
			\item $w_{ij}$ ضرایب ضرب‌های درجه دوم،
			\item $v_i$ ضرایب ترکیب خطی،
			\item $b$ بایاس،
			\item $f(\cdot\cdot\cdot)$ تابع فعال‌سازی.
		\end{itemize}
		
		\item \textbf{نورون کروی (\lr{Spherical / RBF Neuron}):}
		\[
		\mathrm{net}(\mathbf{x})
		= \bigl\lVert \mathbf{x} - \boldsymbol{\mu}\bigr\rVert
		= \sqrt{\sum_{i=1}^n (x_i - \mu_i)^2},
		\quad
		y = f\bigl(\mathrm{net}(\mathbf{x})\bigr),
		\]
		یا گونه‌ی مربعی بدون ریشه:
		\[
		\mathrm{net}(\mathbf{x})
		= \sum_{i=1}^n (x_i - \mu_i)^2,
		\quad
		y = \exp\bigl(-\gamma\,\mathrm{net}(\mathbf{x})\bigr),
		\]
		که در آن
		\begin{itemize}
			\item $\boldsymbol{\mu}=(\mu_1,\dots,\mu_n)$ مرکز نورون،
			\item $\gamma>0$ ضریب پهنای باند،
			\item $f(\cdot)$ می‌تواند تابع خطی یا نمایی باشد.
		\end{itemize}
		
		\item \textbf{نورون چندجمله‌ای (\lr{Polynomial Neuron}):}
		ابتدا ترکیب خطی و توان:
		\[
		u = \sum_{i=1}^n w_i\,x_i + b,
		\quad
		y = u^d
		= \Bigl(\sum_{i=1}^n w_i\,x_i + b\Bigr)^{d}.
		\]
		به‌صورت کلی برای ورودی چندبعدی:
		\[
		y 
		= \sum_{\substack{\alpha_1+\cdots+\alpha_n \le d}}
		w_{\alpha_1,\dots,\alpha_n}
		\;x_1^{\alpha_1}\cdots x_n^{\alpha_n},
		\]
		که در آن
		\begin{itemize}
			\item $d$ درجه چندجمله‌ای،
			\item $\alpha_i\in\mathbb{N}_0$ اندیس‌های چندجمله‌ای،
			\item $w_{\alpha_1,\dots,\alpha_n}$ ضرایب متناظر.
		\end{itemize}
	\end{enumerate}
						%%%%%%%%سوال 5%%%%%%%%5
	\item در این تمرین ساختار دو نوع شبکه عصبی با فیدبک مورد بررسی قرار گرفته است:
	
	\begin{itemize}
		\item \textbf{شبکه عصبی تک‌نورونی با فیدبک به خود}:  
		این نوع شبکه فقط شامل یک نورون است که خروجی آن دوباره به ورودی خودش بازمی‌گردد. این فیدبک باعث می‌شود که رفتار شبکه وابسته به حالت قبلی خروجی باشد. ساختار کلی آن در تصویر زیر نمایش داده شده است:
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.45\textwidth]{./figures/tak.png}
			\caption{شبکه عصبی تک‌نورونی با فیدبک به خود}
		\end{figure}
		
		\item \textbf{شبکه عصبی تک‌لایه با فیدبک (ساختار اول)}:  
		در این ساختار چندین نورون در یک لایه وجود دارند و خروجی یکی از نورون‌ها به‌صورت فیدبک به ورودی خود یا سایر نورون‌ها داده می‌شود. تصویر زیر یکی از حالت‌های ممکن را نشان می‌دهد:
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.50\textwidth]{./figures/tartibi.png}
			\caption{شبکه عصبی تک‌لایه با فیدبک - حالت اول}
		\end{figure}
		
		\item \textbf{شبکه عصبی تک‌لایه با فیدبک (ساختار دوم)}:  
		در این حالت ممکن است فیدبک از خروجی کل شبکه به همه نورون‌ها اعمال شود. تصویر زیر نوعی دیگر از این ساختار را نمایش می‌دهد:
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.55\textwidth]{./figures/total.png}
			\caption{شبکه عصبی تک‌لایه با فیدبک - حالت دوم}
		\end{figure}
		
	\end{itemize}
	
	
	
	


	%%%%%%%%%%%% سوال ۶ %%%%%%%%%%%
	\item 
	
	\begin{enumerate}
		\item {\bf طراحی پرسپترون تک‌لایه}
		
		فرض کنیم می‌خواهیم الگوها را به صورت برچسب
		\(\;t_1=-1\) برای \(P_1\) و \(t_2=+1\) برای \(P_2\) دسته‌بندی کنیم.
		باید \(w\in\mathbb{R}^3\) و بایاس \(b\) را طوری بیابیم که
		\[
		\begin{cases}
			\operatorname{sign}(w^\top P_1 + b) = -1,\\
			\operatorname{sign}(w^\top P_2 + b) = +1.
		\end{cases}
		\]
		این معادلات به صورت نابرابری‌های زیر نوشته می‌شوند:
		\[
		w^\top\!(-1,-1,1) + b < 0,
		\quad
		w^\top\!(+1,-1,1) + b > 0.
		\]
		به سادگی می‌توانیم مثلاً وزن‌ها را به صورت
		\(\;w = (1,0,0)\;\)، و بایاس \(b=0\) انتخاب کنیم:
		\[
		w^\top P_1 + b = -1 < 0,\quad
		w^\top P_2 + b = +1 > 0.
		\]
		لذا تابع تصمیم
		\(\;y = \operatorname{sign}(x_1)\)
		دو الگو را به درستی تفکیک می‌کند.
		
		\item {\bf طراحی شبکه Hamming}
		
		شبکه همینگ برای \(N\) الگو \(P_k\) به صورت زیر است:
		\[
		\textbf{W} = 
		\begin{bmatrix}
			P_1^\top \\ P_2^\top
		\end{bmatrix}, 
		\quad
		y = \arg\max_{k}\bigl(\textbf{W}\,x\bigr)_k.
		\]
		برای \(P_1,P_2\) داریم:
		\[
		\textbf{W} =
		\begin{pmatrix}
			-1 & -1 & 1\\
			+1 & -1 & 1
		\end{pmatrix},
		\quad
		\text{انتخاب \(k\) با بیشینه‌ی \(\sum_i W_{k,i}x_i\).}
		\]
		
		\item {\bf طراحی شبکه Hopfield}
		
		شبکه هاپفیلد با الگوهای باینری \(\pm1\) به کمک قاعده
		\(\;T = \sum_k P_k P_k^\top\;\) ساخته می‌شود. اینجا داریم:
		\[
		T \;=\; P_1P_1^\top + P_2P_2^\top
		\;=\;
		\begin{pmatrix}
			1 & 1 & -1\\
			1 & 1 & -1\\
			-1 & -1 & 1
		\end{pmatrix}
		+
		\begin{pmatrix}
			1 & -1 & 1\\
			-1 & 1 & -1\\
			1 & -1 & 1
		\end{pmatrix}
		=
		\begin{pmatrix}
			2 & 0 & 0\\
			0 & 2 & -2\\
			0 & -2 & 2
		\end{pmatrix}.
		\]
		سپس حالت نرونی‌ها با قاعده
		\(\;x_i \leftarrow \operatorname{sign}\bigl(\sum_j T_{ij}x_j\bigr)\;\)
		به سمت نزدیک‌ترین الگو جذب می‌شود.
		
	\end{enumerate}
	
	%%%%%% سوال ۷ %%%%%%%
	\item
	
	\begin{enumerate}
		\item 
		\textbf{طراحی مرز تصمیم و شبکه پرسپترون تک‌لایه}
		
		
		با انتخاب وزن‌ها و بایاس زیر:
		\[
		\mathbf{w} = \begin{bmatrix}-1\\-1\end{bmatrix},
		\quad b = \frac{1}{2}
		\]
		تابع فعال‌سازی گام به این صورت خواهد بود:
		\[
		y = 
		\begin{cases}
			1, & \mathbf{w}^\top \mathbf{x} + b > 0,\\
			0, & \text{وگرنه}.
		\end{cases}
		\]
		\\
		\textbf{معادله مرز تصمیم:}
		\[
		-x_1 - x_2 + \frac{1}{2} = 0
		\quad\Longleftrightarrow\quad
		x_1 + x_2 = \frac{1}{2}.
		\]
		
		\begin{center}
			\begin{tikzpicture}[scale=1]
				\draw[->] (-2,0) -- (3,0) node[right] {$x_1$};
				\draw[->] (0,-2) -- (0,3) node[above] {$x_2$};
				% نقاط مثبت
				\fill ( -1,  1) circle[radius=2pt] node[above left] {$P_1$};
				\fill (  0,  0) circle[radius=2pt] node[below right] {$P_2$};
				\fill (  1, -1) circle[radius=2pt] node[below right] {$P_3$};
				% نقاط منفی
				\draw[fill=white] ( 1, 0) circle[radius=2pt] node[below right] {$P_4$};
				\draw[fill=white] ( 0, 1) circle[radius=2pt] node[above left] {$P_5$};
				\draw[fill=white] ( 1, 2) circle[radius=2pt] node[right] {$P_6$};
				% مرز تصمیم
				\draw[thick,blue] (-1.5,2) -- (2,-1.5) node[right] {$x_1 + x_2 = \frac{1}{2}$};
			\end{tikzpicture}
		\end{center}
		
		\newpage
		\item 
		\textbf{تشخیص قابلیت جداسازی و تعیین بازه \(\varepsilon\)}
		
		از‌ نامعادلات زیر برای کلاس بندی استفاده می‌کنیم:
		\[
		\begin{cases}
			-x_1 - x_2 + b > 0 & 1\\
			-x_1 - x_2 + b < 0 & 0
		\end{cases}
		\]
		نتیجه می‌شود که برای هر \(\varepsilon\ge0\) می‌توان \(w_1=w_2=-1\) و \(b\in(0,1)\) (مثلاً \(b=1\)) را انتخاب کرد و جداسازی خطی امکان‌پذیر است.
		
		\item 
		\textbf{اجرای الگوریتم پرسپترون و نتایج نهایی}
		
		برای سه مقدار \(\varepsilon\) اجرای الگوریتم با نرخ یادگیری \(\eta=1\)، وزن و بایاس را از صفر مقداردهی کرده و تا خطای صفر تکرار می‌کنیم.
		
		\lstinputlisting[language=Python, caption= پیاده‌سازی الگوریتم پرسپترون]{scripts/epsilon.py}

		
		{\small
			\[
			\begin{aligned}
				&\varepsilon=1:\quad \mathbf{w}=(-1,\,-1),\; b=1,\;\text{epochs}=2,\\
				&\varepsilon=2:\quad \mathbf{w}=(-2,\,-2),\; b=1,\;\text{epochs}=4,\\
				&\varepsilon=6:\quad \mathbf{w}=(-3,\,-4),\; b=3,\;\text{epochs}=4.
			\end{aligned}
			\]}
		
		\item 
		\textbf{خلاصه نتایج}
		
		\begin{itemize}
			\item مرز تصمیم: \(x_1 + x_2 = \frac{1}{2}\).
			\item بازه \(\varepsilon\): \(\varepsilon \ge 0\) (تمام مقادیر غیرمنفی).
			\item وزن‌ها و بایاس نهایی برای \(\varepsilon=1,2,6\) مطابق جدول فوق.
			\item الگوریتم پرسپترون حداکثر تا ۴ دور همگرا شده و خطای صفر حاصل شد.
		\end{itemize}
		
	\end{enumerate}

						%%%%%%%%%%سوال 8%%%%%%%%%%%%%%%


	\item  
	\begin{enumerate}
		\item نمایش تصویری هر الگو به صورت یک تصویر \(3\times3\) دودویی:
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.6\textwidth]{./figures/question8a.png}
			\caption{الگوهای \(P_1\) تا \(P_6\) به صورت پیکسل‌های سفید و سیاه}
			\label{fig:question8a}
		\end{figure}
		\item 
		\begin{itemize}
			\item برای تشخیص الگوهای $P_1$ تا $P_6$ از یکدیگر با استفاده از یک پرسپترون تک‌لایه با یک نرون، نیاز داریم که هر الگو را به صورت یک بردار ورودی مناسب برای نرون تبدیل کنیم. از آنجا که هر الگو یک ماتریس $3\times3$ است، می‌توانیم آن را به یک بردار $1\times9$ تبدیل کنیم. برای سادگی، فرض می‌کنیم که ماتریس را به صورت سطر به سطر به یک بردار 9 عنصری تبدیل می‌کنیم.
			\item تبدیل الگوها به بردارهای ورودی:
			\begin{align*}
				P_1 &= [1,1,1,-1,-1,-1,-1,-1,-1],\\
				P_2 &= [-1,-1,-1,1,1,1,-1,-1,-1],\\
				P_3 &= [-1,-1,-1,-1,-1,-1,-1,1,1],\\
				P_4 &= [1,-1,-1,1,-1,-1,1,-1,-1],\\
				P_5 &= [-1,1,-1,1,-1,1,-1,1,-1],\\
				P_6 &= [-1,-1,1,-1,-1,1,-1,-1,1].
			\end{align*}
			\item هدف: جداسازی دو کلاس با پرسپترون تک‌نرونی:
			\begin{itemize}
				\item کلاس 1: $P_1, P_2, P_3$ (تارگت $t=1$)
				\item کلاس 0: $P_4, P_5, P_6$ (تارگت $t=0$)
			\end{itemize}
			\item مشخصات پرسپترون:
			\begin{itemize} 
				\item تعداد ورودی: 9
				\item تعداد نرون: 1
				\item تابع فعال‌سازی: تابع پله با آستانه صفر
				\item خروجی $y=\begin{cases}1,&\text{اگر }\mathrm{net}\ge0,\\0,&\text{اگر }\mathrm{net}<0.\end{cases}$
			\end{itemize}
			\item مقادیر اولیه:
			\begin{itemize}
				\item وزن‌ها: $w^{(0)}=[0,0,0,0,0,0,0,0,0]$
				\item بایاس: $b^{(0)}=0$
				\item نرخ یادگیری: $\eta=1$
			\end{itemize}
			\item قانون به‌روزرسانی:
			\begin{align*}
				w^{(\text{new})} &= w^{(\text{old})} + \eta\,(t - y)\,x,\\
				b^{(\text{new})} &= b^{(\text{old})} + \eta\,(t - y).
			\end{align*}
			\item مراحل یادگیری (اپوک‌ها):
			\begin{enumerate}
				\item اپوک 1:
				\begin{itemize}
					\item الگو $P_1$: محاسبه $\mathrm{net}=0$, $y=1$, خطا $e=0$ (بدون تغییر)
					\item الگو $P_2$ و $P_3$: مشابه، خطا صفر (بدون تغییر)
					\item الگو $P_4$: $\mathrm{net}=0$, $y=1$, $e=-1$, به‌روزرسانی:
					\begin{align*}
						w &= w + (-1)\,P_4 = [-1,1,1,-1,1,1,-1,1,1],\\
						b &= -1.
					\end{align*}
					\item الگو $P_5$: محاسبه $\mathrm{net}=0$, $y=1$, $e=-1$, به‌روزرسانی:
					\begin{align*}
						w &= [0,0,2,-2,2,0,0,0,2],\\
						b &= -2.
					\end{align*}
					\item الگو $P_6$: $\mathrm{net}=2$, $y=1$, $e=-1$, به‌روزرسانی تا پایان اپوک 1:
					\begin{align*}
						w &= [1,1,1,-1,3,-1,1,1,1],\\
						b &= -3.
					\end{align*}
				\end{itemize}
				\item اپوک 2:
				\begin{itemize}
					\item ... (ادامه تا همگرایی)
				\end{itemize}
				\item \dots
				\item اپوک 6 (نهایی): وزن‌ها و بایاس نهایی:
				\[ w = [0,0,-2,-2,6,-4,-4,0,-2],\quad b = 0. \]
			\end{enumerate}
			\item نحوه تشخیص الگوها: با وزن‌ها و بایاس نهایی، خروجی 1 برای $P_1,P_2,P_3$ و خروجی 0 برای $P_4,P_5,P_6$ خواهد بود.
		\end{itemize}
	\end{enumerate}
	طراحی یک پرسپترون تک‌لایه با یک نورون برای جدا کردن این الگوها:
	
	\begin{itemize}
		\item وزن‌ها و بایاس اولیه:
		\[
		w_1 = w_2 = \cdots = w_9 = 0,\quad b = 0.
		\]
		\item تابع فعال‌سازی: 
		\(\displaystyle
		y = \mathrm{sign}\bigl(\mathbf{w}^\top \mathbf{x} + b\bigr)
		\in\{+1,-1\}.
		\)
		\item قانون یادگیری پرسپترون:
		\[
		w_i \leftarrow w_i + \eta\,(d - y)\,x_i,
		\quad
		b \leftarrow b + \eta\,(d - y),
		\]
		که \(\eta\) نرخ یادگیری، \(d\) برچسب هدف و \(x_i\) مؤلفهٔ \(i\)ام ورودی است.
	\end{itemize}
	
	\lstinputlisting[language=Python, caption=پیاده‌سازی پرسپترون برای سوال ۸]{./scripts/perceptron8.py}
	
	در پایان آموزش تا همگرایی، وزن‌ها و بایاس نهایی به صورت زیر به دست می‌آیند:
	\[
	\mathbf{w}_{\mathrm{final}} = [\,w_1^*, w_2^*, \dots, w_9^*\,],
	\quad
	b_{\mathrm{final}} = b^*.
	\]
	
	
						%%%%%%%%سوال 9%%%%%%%%%%%%%%
	\item  
	برای تقسيم $\mathbb{R}^2$ به $m$ ناحيهٔ مجزا با يک شبکهٔ دو لايه، بايد $k$ نورون در لايهٔ پنهان داشته باشيم به طوري که حداکثر تعداد ناحيه‌هاي ايجادشده توسط $k$ خط برابر باشد با:
	\[
	N(k)=\frac{k(k+1)}{2}+1.
	\]
	پس بايد عدد $k$ کمترين عدد صحيح باشد که
	\[
	N(k)\ge m\quad\Longrightarrow\quad \frac{k(k+1)}{2}+1\ge m.
	\]
	با حل اين نامعادله، خواهيم داشت:
	\[
	k^2 + k -2(m-1)\ge 0
	\quad\Longrightarrow\quad k\ge \frac{-1+\sqrt{1+8(m-1)}}{2}.
	\]
	بنابراين حداقل تعداد نورون‌هاي لايهٔ پنهان:
	\[
	k=\left\lceil\frac{-1+\sqrt{8m-7}}{2}\right\rceil.
	\]
	
	\vspace{1em}
	\noindent
	\textbf{اثبات فرمول حداکثر تعداد ناحیه‌ها با استفاده از استقرا:}
	
	\begin{description}
		\item[قضيه:] با $k$ خط در صفحه حداکثر $N(k)=k(k+1)/2+1$ ناحيه متناظر ايجاد مي‌شود.
		\item[پايهٔ استقرا ($k=0$):] با صفر خط در صفحه، تمام صفحه يک ناحيهٔ يکپارچه است. بنابراين
		\[
		N(0)=\frac{0\cdot1}{2}+1=1,
		\]
		که صحيح است.
		\item[فرض استقرا:] فرض کنيم براي $k$ خط، فرمول
		\[
		N(k)=\frac{k(k+1)}{2}+1
		\]
		برقرار باشد.
		\item[گام استقرا ($k\to k+1$):]
		اگر يک خط تازه به مجموعهٔ $k$ خط اضافه کنيم، اين خط جديد با هر يک از $k$ خط قبلي در يک نقطه تلاقی مي‌کند، بنابراين در مجموع $k$ نقطهٔ تلاقی ايجاد مي‌شود. اين $k$ نقطه، خط جديد را به $k+1$ قطعه تقسيم مي‌کند. هر قطعه يک ناحيهٔ جديد ايجاد مي‌کند. بنابراين:
		\[
		N(k+1)=N(k)+(k+1).
		\]
		با جاگذاري فرض استقرايي داريم:
		\[
		N(k+1)=\frac{k(k+1)}{2}+1+(k+1)
		=\frac{k(k+1)+2(k+1)}{2}+1
		=\frac{(k+1)(k+2)}{2}+1.
		\]
		بنابراين فرمول براي $k+1$ نيز برقرار است.
	\end{description}
	
	\noindent
	اين اثبات کامل است و نتيجه مي‌دهد که براي تقسيم $\mathbb{R}^2$ به $m$ ناحيه، حداقل
	\[
	\left\lceil\tfrac{-1+\sqrt{8m-7}}{2}\right\rceil
	\]
	نورون در لايهٔ پنهان نياز داريم.

	
	
\end{enumerate}
